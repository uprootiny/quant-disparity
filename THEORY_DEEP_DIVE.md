# Theory Deep Dive: What We Actually Have

## Logical Structure

```
                    EMPIRICAL AXIOMS (observed, not proved)
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ T-009: r(Îº_eff, D) = -0.991        â”‚
                    â”‚ T-010: slope = -ln(2)/2            â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚ validates
                                    â–¼
                    THEORETICAL CLAIMS (formalized, sorry)
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Disparity Bound: D â‰¤ CÂ·âˆšVar[Îº_eff]Â·2^{-B}            â”‚
        â”‚ Rate-Distortion: MSE âˆ 2^{-B/2}                       â”‚
        â”‚ Monotonicity: âˆ‚Î±*/âˆ‚Îº > 0                              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚ depends on
                                    â–¼
                    MSE DECOMPOSITION (Banner foundation)
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ MSE(Î±) = E_clip(Î±) + E_quant(Î±)                      â”‚
        â”‚ Convexity: MSE is convex â‡’ unique Î±*                 â”‚
        â”‚ Trade-off: â†“clip â‡” â†‘quant                            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚ depends on
                                    â–¼
                    MIXTURE KURTOSIS (LA-ACIQ extension)
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Îº_eff = [Î£wáµ¢(Îºáµ¢+3)Ïƒáµ¢â´ + 6Î£wáµ¢Ïƒáµ¢Â²Î´áµ¢Â² + Î£wáµ¢Î´áµ¢â´]/Ïƒâ´-3 â”‚
        â”‚ Language-specific: wáµ¢ = Äáµ¢(Î») (activation fraction)  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚ depends on
                                    â–¼
                    CLIPPING (fully proved in Lean)
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ clip(x,Î±) = max(-Î±, min(Î±, x))                       â”‚
        â”‚ 9 theorems PROVED: range, idempotent, monotone, etc. â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Layer 1: Clipping (SOLID - 9 Theorems Proved)

**Status:** Complete formal verification

| Theorem | Statement | Status |
|---------|-----------|--------|
| `clip_le_alpha` | clip(x,Î±) â‰¤ Î± | **PROVED** |
| `neg_alpha_le_clip` | -Î± â‰¤ clip(x,Î±) | **PROVED** |
| `clip_in_range` | -Î± â‰¤ clip(x,Î±) â‰¤ Î± | **PROVED** |
| `clip_of_in_range` | xâˆˆ[-Î±,Î±] â†’ clip(x,Î±)=x | **PROVED** |
| `clip_idempotent` | clip(clip(x,Î±),Î±) = clip(x,Î±) | **PROVED** |
| `clip_abs_le` | \|clip(x,Î±)\| â‰¤ Î± | **PROVED** |
| `clip_mono_x` | xâ‰¤y â†’ clip(x,Î±)â‰¤clip(y,Î±) | **PROVED** |
| `clip_mono_alpha` | Î±â‰¤Î² â†’ clip(x,Î±)âˆˆ[-Î²,Î²] | **PROVED** |
| `clip_nonneg` | xâ‰¥0 â†’ clip(x,Î±)â‰¥0 | **PROVED** |

**What this gives us:** The fundamental operation of quantization is well-defined and behaves correctly.

---

## Layer 2: MSE Decomposition (SCAFFOLDED)

**Status:** Formalized in Lean with `sorry`, validated numerically

### Definitions (correct)

```lean
-- Clipping error: E[(|X| - Î±)Â² Â· ğŸ™_{|X| > Î±}]
def clippingError (Î± : â„) : â„ :=
  âˆ« Ï‰, (|X Ï‰| - Î±)^2 * (if |X Ï‰| > Î± then 1 else 0) âˆ‚Î¼

-- Quantization noise: Î”Â²/12 Â· P(|X| â‰¤ Î±)
def quantizationNoise (Î± : â„) (B : BitWidth) : â„ :=
  (stepSize Î± B)^2 / 12 * (Î¼ {Ï‰ | |X Ï‰| â‰¤ Î±}).toReal

-- Total MSE
def mse (Î± : â„) (B : BitWidth) : â„ :=
  âˆ« Ï‰, (quantError (X Ï‰) Î± B)^2 âˆ‚Î¼
```

### Theorems (sorry)

| Theorem | Statement | Status |
|---------|-----------|--------|
| `mse_decomposition` | MSE = E_clip + E_quant | `sorry` |
| `clippingError_antitone` | âˆ‚E_c/âˆ‚Î± < 0 | `sorry` |
| `quantizationNoise_monotone` | âˆ‚E_q/âˆ‚Î± > 0 | `sorry` |
| `mse_convex` | MSE convex for Î±>0 | `sorry` |

**Proof sketch for MSE decomposition:**
1. Split integral: inside vs outside [-Î±, Î±]
2. Inside: only quantization error contributes
3. Outside: only clipping error contributes
4. Quantization error is uniformly distributed on [-Î”/2, Î”/2], variance = Î”Â²/12

**Why not proved:** Requires careful measure-theoretic argument with indicator functions and conditional expectations.

---

## Layer 3: Mixture Kurtosis (SCAFFOLDED)

**Status:** Formula correct, algebraic simplification not verified

### The Key Formula

```
Îº_eff(M) = [Î£áµ¢ wáµ¢(Îºáµ¢+3)Ïƒáµ¢â´ + 6Î£áµ¢ wáµ¢Ïƒáµ¢Â²Î´áµ¢Â² + Î£áµ¢ wáµ¢Î´áµ¢â´] / Ïƒ_effâ´ - 3
```

Where:
- `wáµ¢` = mixture weight (activation fraction for language)
- `Îºáµ¢` = excess kurtosis of component i
- `Ïƒáµ¢` = standard deviation of component i
- `Î´áµ¢` = deviation of component mean from mixture mean
- `Ïƒ_effÂ²` = mixture variance = Î£wáµ¢Ïƒáµ¢Â² + Î£wáµ¢Î´áµ¢Â²

### Derivation (standard probability theory)

1. **Law of total variance:** Var(X) = E[Var(X|Y)] + Var(E[X|Y])
2. **Fourth moment:** Expand E[(X-Î¼)â´] using mixture structure
3. **Cross terms:** The 6Î£wáµ¢Ïƒáµ¢Â²Î´áµ¢Â² comes from E[(X-Î¼)Â²(Î¼áµ¢-Î¼)Â²]

This is textbook material (see FrÃ¼hwirth-Schnatter "Finite Mixture Models").

---

## Layer 4: Optimal Clipping (APPROXIMATION)

**Status:** Banner approximation, empirically validated

### Banner's Formula

```
Î±*/Ïƒ â‰ˆ 2.5 + 0.3Â·ln(1 + max(0,Îº))    (for INT4)
```

### Origin

Banner et al. (2019) derived this by:
1. Taking derivative of MSE: dMSE/dÎ± = dE_clip/dÎ± + dE_quant/dÎ± = 0
2. For Gaussian: Î±*/Ïƒ â‰ˆ 2.5
3. Empirical fit for other distributions: add 0.3Â·ln(1+Îº) correction

### What we extended

**LA-ACIQ:** Use language-specific Îº_eff(Î») instead of global Îº

```
Î±*(Î») = Ïƒ_eff(Î») Â· (2.5 + 0.3Â·ln(1 + max(0, Îº_eff(Î»))))
```

---

## Layer 5: Disparity Bound (CONJECTURED)

**Status:** Empirically plausible, not proved

### The Claim

```
max_Î» MSE(Î») - min_Î» MSE(Î») â‰¤ C Â· âˆšVar_Î»[Îº_eff(Î»)] Â· 2^{-B}
```

### Intuition

1. **Kurtosis variation** causes variation in optimal Î±*
2. Using global Î±* (not per-language) creates suboptimality
3. Suboptimality âˆ distance from optimal â‰ˆ Îº_eff - Îº_global
4. Variance in Îº_eff determines worst-case gap

### Empirical fit

From `spec.json`:
- Observed disparity / âˆšVar[Îº_eff] â‰ˆ 0.015
- This gives C â‰ˆ 0.015

**Problem:** We fit C from data, then "validate" it fits. Circular.

---

## Layer 6: Empirical Axioms (VALIDATED)

**Status:** Strong correlation, but possibly circular

### T-009: Kurtosis-Degradation Correlation

```
r(Îº_eff, degradation) = -0.991, p < 0.001
```

**Interpretation:** Languages with higher effective kurtosis (heavier tails in their activated weight distribution) experience LESS degradation.

Wait - this seems backwards. Let me check...

Actually: negative correlation means higher Îº_eff â†’ LOWER degradation. This makes sense because:
- Higher Îº means heavier tails
- Banner approximation gives larger Î± for higher Îº
- Larger Î± â†’ less clipping error for heavy-tailed data
- So if global Î± is used, high-Îº languages benefit, low-Îº suffer

### T-010: Rate-Distortion Slope

```
slope = -ln(2)/2 â‰ˆ -0.347
```

**Origin:** Shannon's Gaussian rate-distortion function D(R) = ÏƒÂ²Â·2^{-2R}

Taking log: log(D) = log(ÏƒÂ²) - 2RÂ·log(2) = const - RÂ·ln(2)

For quantization, R â‰ˆ B (bits), so:
- log(D) vs B has slope -2Â·ln(2) for MSE
- For relative degradation (ratio), slope is -ln(2)/2

---

## What's Actually Proved vs Assumed

### PROVED (machine-checked)
- All 9 clipping properties
- Definitions type-check in Lean

### FORMALIZED (sorry)
- MSE decomposition
- MSE convexity
- Kurtosis formula
- Monotonicity âˆ‚Î±*/âˆ‚Îº > 0
- Disparity bound structure

### EMPIRICALLY VALIDATED
- T-009: Îº_eff correlation (r = -0.991)
- T-010: Rate-distortion slope (-0.347)
- T-003: Gateway layer variance (3.08x)
- T-004: L0+L11 synergy (0.992 similarity)

### ASSUMED
- Banner approximation accuracy (cited, not proved)
- Activation fractions approximate mixture weights
- Redundancy â†” disparity relationship

---

## Critical Gaps

### 1. MSE Convexity Proof

**Why it matters:** Ensures unique optimal Î± exists.

**Proof approach:**
1. E_clip is convex (second derivative â‰¥ 0)
2. E_quant is convex (quadratic in Î± via Î”)
3. Sum of convex is convex

**Difficulty:** Requires showing âˆ‚Â²E_clip/âˆ‚Î±Â² â‰¥ 0, which involves the distribution tail.

### 2. Monotonicity Proof

**Why it matters:** Justifies Banner's approximation trend.

**Proof approach:**
1. Implicit function theorem on first-order condition
2. Show âˆ‚Î±*/âˆ‚Îº = -âˆ‚Â²MSE/âˆ‚Î±âˆ‚Îº / âˆ‚Â²MSE/âˆ‚Î±Â² > 0

**Difficulty:** Need explicit form of MSE dependence on Îº.

### 3. Disparity Bound Derivation

**Why it matters:** Would give theoretical guarantee, not just empirical fit.

**Proof approach:**
1. Taylor expand MSE around optimal Î±*
2. Use Îº variation to bound Î±* variation
3. Convert Î±* variation to MSE variation

---

## The Honest Picture

| Component | Confidence | Evidence |
|-----------|------------|----------|
| Clipping properties | **100%** | Machine-checked |
| MSE decomposition | **95%** | Standard, well-known |
| MSE convexity | **90%** | Intuitive, numerically verified |
| Mixture kurtosis formula | **95%** | Textbook result |
| Banner approximation | **85%** | Published, cited 500+ times |
| LA-ACIQ extension | **70%** | Novel, but follows naturally |
| Îº_eff correlation | **80%** | Strong signal, but possibly circular |
| Disparity bound | **50%** | Empirical fit, not derived |

---

## What Would Make This Rigorous

1. **Complete MSE convexity proof** â†’ establishes optimization is well-posed
2. **Prove monotonicity** â†’ justifies kurtosis-based reasoning
3. **Derive C from first principles** â†’ removes circular validation
4. **Real GPU experiments** â†’ breaks simulation circularity
