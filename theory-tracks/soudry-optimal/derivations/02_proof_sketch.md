# LA-ACIQ Proof Sketch

*Based on validated T-009 and T-010 results*

---

## Goal

Prove that for language Î» with effective kurtosis Îº_eff(Î»), the optimal clipping threshold is:

$$\alpha^*(\lambda) = \sigma_{\text{eff}}(\lambda) \cdot g(\kappa_{\text{eff}}(\lambda), B)$$

where g is a monotonically increasing function in Îº.

---

## Theorem 1: MSE Decomposition for Mixtures

**Statement.** For X ~ P_Î» = Î£_l Ä_l(Î») Â· P_l, the MSE under clipped quantization Q_Î± decomposes as:

$$\text{MSE}_\lambda(\alpha) = E_c^\lambda(\alpha) + E_q^\lambda(\alpha)$$

where:
- E_c^Î»(Î±) = E[(|X| - Î±)Â² Â· ğŸ™_{|X|>Î±}] (clipping error)
- E_q^Î»(Î±) = Î”Â²/12 Â· P(|X| â‰¤ Î±) (quantization noise)

**Proof sketch:**

1. Quantization error decomposes into clipping + noise (Banner 2019, Theorem 1)
2. For mixture: E_Î»[f(X)] = Î£_l Ä_l(Î») Â· E_l[f(X)]
3. Each component contributes additively
4. Sum preserves the decomposition structure âˆ

---

## Theorem 2: Convexity of MSE_Î»(Î±)

**Statement.** MSE_Î»(Î±) is convex in Î± for Î± > 0.

**Proof sketch:**

1. E_c^Î»(Î±) is convex: second derivative â‰¥ 0
   - âˆ‚E_c/âˆ‚Î± = -2Î± Â· P(|X| > Î±) + âˆ«_{|x|>Î±} 2(|x|-Î±)Â·(-1) dx
   - âˆ‚Â²E_c/âˆ‚Î±Â² = ... â‰¥ 0 (algebra)

2. E_q^Î»(Î±) = (2Î±)Â²/(12Â·(2^B-1)Â²) Â· P(|X| â‰¤ Î±)
   - Quadratic in Î±, hence convex

3. Sum of convex functions is convex âˆ

---

## Theorem 3: Optimal Clipping Depends on Kurtosis

**Statement.** Let Î±*(Îº) denote the optimal clipping for a distribution with kurtosis Îº. Then:

$$\frac{\partial \alpha^*}{\partial \kappa} > 0$$

**Proof sketch:**

1. Higher Îº â†’ heavier tails â†’ more probability mass at extremes
2. Clipping error E_c more sensitive to Î± when tails are heavy
3. Optimal Î±* shifts outward to reduce clipping error
4. Formally: use implicit function theorem on âˆ‚MSE/âˆ‚Î± = 0 âˆ

---

## Theorem 4: Effective Kurtosis Formula

**Statement.** For mixture P_Î» = Î£_l Ä_l(Î») Â· P_l with component means Î¼_l, variances Ïƒ_lÂ², and kurtoses Îº_l:

$$\kappa_{\text{eff}}(\lambda) = \frac{\sum_l \bar{a}_l(\lambda) \cdot (\mu_{4,l} + 6\sigma_l^2 \delta_l^2 + 3\delta_l^4)}{\sigma_{\text{eff}}^4(\lambda)} - 3$$

where Î´_l = Î¼_l - Î¼_eff(Î») and Î¼â‚„,l is the 4th central moment of component l.

**Proof sketch:**

1. 4th moment of mixture: E[Xâ´] = Î£_l Ä_l E_l[Xâ´]
2. Expand E_l[(X - Î¼_eff)â´] using binomial
3. Collect terms involving component moments
4. Divide by Ïƒ_effâ´ and subtract 3 âˆ

---

## Theorem 5: Disparity Bound

**Statement.** Under LA-ACIQ with per-language Î±*(Î»):

$$\max_\lambda \text{MSE}_\lambda - \min_\lambda \text{MSE}_\lambda \leq C \cdot \text{Var}_\lambda[\kappa_{\text{eff}}(\lambda)]^{1/2} \cdot 2^{-B}$$

for some constant C depending on the model.

**Proof sketch:**

1. MSE_Î»(Î±*(Î»)) depends continuously on Îº_eff(Î»)
2. Taylor expand MSE around mean ÎºÌ„:
   MSE_Î» â‰ˆ MSE(ÎºÌ„) + (âˆ‚MSE/âˆ‚Îº)(Îº_eff(Î») - ÎºÌ„)
3. Max-min â‰¤ 2Â·|âˆ‚MSE/âˆ‚Îº|Â·max|Îº_eff - ÎºÌ„|
4. Rate-distortion gives 2^{-B} scaling âˆ

---

## Corollary: Rate-Distortion Slope

**Statement.** The disparity-vs-bits relationship has slope -log(2)/2.

**Proof:**

From T-010 validation: slope = -0.347 â‰ˆ -ln(2)/2 = -0.347

This matches the Gaussian rate-distortion bound D(R) = ÏƒÂ² Â· 2^{-2R}, confirming that quantization error follows Shannon's fundamental limit.

---

## Empirical Validation

From T-009:
- Predicted: Îº_eff â†” degradation correlation
- Observed: r = -0.991, p = 1.84 Ã— 10â»â¶

From T-010:
- Predicted: disparity âˆ 2^{-B/2}
- Observed: RÂ² = 1.0

Both core predictions validated.

---

## What Remains

1. **Algebra:** Complete âˆ‚Â²MSE/âˆ‚Î±Â² calculation
2. **Bound tightness:** Determine if C is achievable
3. **Closed form:** Solve âˆ‚MSE/âˆ‚Î± = 0 for specific distributions
4. **Computation:** Efficient algorithm for Î±*(Î») in practice

---

*Proof Sketch â€” 2026-01-11*
