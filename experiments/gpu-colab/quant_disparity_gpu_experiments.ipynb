{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Disparity: GPU Experiments\n",
    "\n",
    "**Purpose:** Validate simulation findings with real models\n",
    "\n",
    "**Priority Order (Epistemic Value):**\n",
    "1. **G1: Real Tokenization Analysis** - Verify alignment metric assumptions\n",
    "2. **G2: True Quantization Effects** - Validate degradation patterns\n",
    "3. **G3: Layer Importance Probing** - Confirm gateway hypothesis\n",
    "\n",
    "**Output Format:** Results are formatted for copy-paste back to local analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SETUP ===\n",
    "# Run this cell first to install dependencies\n",
    "\n",
    "!pip install -q transformers accelerate bitsandbytes sentencepiece\n",
    "!pip install -q torch --upgrade\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU CHECK\")\n",
    "print(\"=\" * 60)\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Memory: {gpu_mem:.1f} GB\")\n",
    "    print(\"Status: READY\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Go to Runtime > Change runtime type > GPU\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## G1: Real Tokenization Analysis\n",
    "\n",
    "**Question:** Does our alignment metric match real tokenizer behavior?\n",
    "\n",
    "**Epistemic Value:** HIGH - validates core assumption\n",
    "\n",
    "**Method:**\n",
    "1. Load Llama-2 tokenizer\n",
    "2. Tokenize parallel sentences in multiple languages\n",
    "3. Compute token-to-morpheme ratios\n",
    "4. Compare to our simulated alignment values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === G1: REAL TOKENIZATION ANALYSIS ===\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "\n",
    "print(\"Loading Llama-2 tokenizer...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", use_fast=True)\n",
    "except:\n",
    "    print(\"Llama-2 requires access. Falling back to GPT-2 tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Parallel test sentences (same meaning, different languages)\n",
    "TEST_SENTENCES = {\n",
    "    'en': [\n",
    "        \"The computer processes data quickly.\",\n",
    "        \"Scientists discovered new evidence.\",\n",
    "        \"The weather is beautiful today.\",\n",
    "        \"Students learn mathematics in school.\",\n",
    "        \"The government announced new policies.\",\n",
    "    ],\n",
    "    'de': [\n",
    "        \"Der Computer verarbeitet Daten schnell.\",\n",
    "        \"Wissenschaftler entdeckten neue Beweise.\",\n",
    "        \"Das Wetter ist heute wunderschön.\",\n",
    "        \"Schüler lernen Mathematik in der Schule.\",\n",
    "        \"Die Regierung kündigte neue Maßnahmen an.\",\n",
    "    ],\n",
    "    'fr': [\n",
    "        \"L'ordinateur traite les données rapidement.\",\n",
    "        \"Les scientifiques ont découvert de nouvelles preuves.\",\n",
    "        \"Le temps est magnifique aujourd'hui.\",\n",
    "        \"Les étudiants apprennent les mathématiques à l'école.\",\n",
    "        \"Le gouvernement a annoncé de nouvelles politiques.\",\n",
    "    ],\n",
    "    'he': [\n",
    "        \"המחשב מעבד נתונים במהירות.\",\n",
    "        \"מדענים גילו ראיות חדשות.\",\n",
    "        \"מזג האוויר יפה היום.\",\n",
    "        \"תלמידים לומדים מתמטיקה בבית הספר.\",\n",
    "        \"הממשלה הכריזה על מדיניות חדשה.\",\n",
    "    ],\n",
    "    'ar': [\n",
    "        \"يقوم الحاسوب بمعالجة البيانات بسرعة.\",\n",
    "        \"اكتشف العلماء أدلة جديدة.\",\n",
    "        \"الطقس جميل اليوم.\",\n",
    "        \"يتعلم الطلاب الرياضيات في المدرسة.\",\n",
    "        \"أعلنت الحكومة عن سياسات جديدة.\",\n",
    "    ],\n",
    "    'zh': [\n",
    "        \"计算机快速处理数据。\",\n",
    "        \"科学家发现了新证据。\",\n",
    "        \"今天天气很好。\",\n",
    "        \"学生在学校学习数学。\",\n",
    "        \"政府宣布了新政策。\",\n",
    "    ],\n",
    "    'ja': [\n",
    "        \"コンピュータはデータを素早く処理します。\",\n",
    "        \"科学者たちは新しい証拠を発見しました。\",\n",
    "        \"今日は天気がいいです。\",\n",
    "        \"生徒は学校で数学を学びます。\",\n",
    "        \"政府は新しい政策を発表しました。\",\n",
    "    ],\n",
    "    'ko': [\n",
    "        \"컴퓨터가 데이터를 빠르게 처리합니다.\",\n",
    "        \"과학자들이 새로운 증거를 발견했습니다.\",\n",
    "        \"오늘 날씨가 아름답습니다.\",\n",
    "        \"학생들은 학교에서 수학을 배웁니다.\",\n",
    "        \"정부가 새로운 정책을 발표했습니다.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Expected word counts (approximate morpheme counts)\n",
    "EXPECTED_WORDS = {\n",
    "    'en': [5, 4, 5, 5, 5],\n",
    "    'de': [5, 4, 5, 6, 6],\n",
    "    'fr': [5, 6, 5, 7, 7],\n",
    "    'he': [4, 4, 4, 5, 5],\n",
    "    'ar': [5, 4, 3, 5, 5],\n",
    "    'zh': [4, 4, 4, 5, 4],\n",
    "    'ja': [5, 5, 4, 5, 5],\n",
    "    'ko': [4, 4, 3, 5, 4],\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"G1: TOKENIZATION ANALYSIS RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for lang, sentences in TEST_SENTENCES.items():\n",
    "    token_counts = []\n",
    "    for sent in sentences:\n",
    "        tokens = tokenizer.encode(sent)\n",
    "        token_counts.append(len(tokens))\n",
    "    \n",
    "    avg_tokens = np.mean(token_counts)\n",
    "    avg_words = np.mean(EXPECTED_WORDS.get(lang, [5]*5))\n",
    "    \n",
    "    # Token-to-word ratio (inverse of alignment)\n",
    "    ratio = avg_tokens / avg_words\n",
    "    \n",
    "    # Compute alignment proxy (higher = better tokenization)\n",
    "    alignment_proxy = 1 / ratio if ratio > 0 else 0\n",
    "    \n",
    "    results[lang] = {\n",
    "        'avg_tokens': avg_tokens,\n",
    "        'avg_words': avg_words,\n",
    "        'token_word_ratio': ratio,\n",
    "        'alignment_proxy': alignment_proxy,\n",
    "        'token_counts': token_counts,\n",
    "    }\n",
    "    \n",
    "    print(f\"{lang}: tokens={avg_tokens:.1f}, words={avg_words:.1f}, ratio={ratio:.2f}, align={alignment_proxy:.3f}\")\n",
    "\n",
    "# Output in parseable format\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"G1_RESULTS_JSON_START\")\n",
    "print(json.dumps(results, indent=2))\n",
    "print(\"G1_RESULTS_JSON_END\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## G2: True Quantization Effects\n",
    "\n",
    "**Question:** Does quantization affect languages differently in real models?\n",
    "\n",
    "**Epistemic Value:** CRITICAL - core hypothesis validation\n",
    "\n",
    "**Method:**\n",
    "1. Load small model (GPT-2 or similar)\n",
    "2. Compute perplexity on parallel sentences (FP32)\n",
    "3. Quantize to INT8 and INT4\n",
    "4. Compare degradation across languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === G2: TRUE QUANTIZATION EFFECTS ===\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "\n",
    "print(\"Loading model for quantization experiment...\")\n",
    "print(\"(Using GPT-2 for accessibility - Llama requires auth)\")\n",
    "\n",
    "model_name = \"gpt2\"  # Small, accessible model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load FP32 model\n",
    "model_fp32 = AutoModelForCausalLM.from_pretrained(model_name).cuda()\n",
    "model_fp32.eval()\n",
    "\n",
    "def compute_perplexity(model, tokenizer, text):\n",
    "    \"\"\"Compute perplexity for a given text.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "    \n",
    "    return torch.exp(loss).item()\n",
    "\n",
    "# Test sentences (using G1 sentences)\n",
    "TEST_SENTENCES = {\n",
    "    'en': \"The computer processes data quickly and efficiently.\",\n",
    "    'de': \"Der Computer verarbeitet Daten schnell und effizient.\",\n",
    "    'fr': \"L'ordinateur traite les données rapidement et efficacement.\",\n",
    "    'he': \"המחשב מעבד נתונים במהירות וביעילות.\",\n",
    "    'ar': \"يقوم الحاسوب بمعالجة البيانات بسرعة وكفاءة.\",\n",
    "    'zh': \"计算机快速高效地处理数据。\",\n",
    "    'ja': \"コンピュータはデータを迅速かつ効率的に処理します。\",\n",
    "    'ko': \"컴퓨터는 데이터를 빠르고 효율적으로 처리합니다.\",\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"G2: QUANTIZATION EFFECTS - FP32 BASELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fp32_results = {}\n",
    "for lang, text in TEST_SENTENCES.items():\n",
    "    try:\n",
    "        ppl = compute_perplexity(model_fp32, tokenizer, text)\n",
    "        fp32_results[lang] = ppl\n",
    "        print(f\"{lang}: PPL = {ppl:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{lang}: ERROR - {e}\")\n",
    "        fp32_results[lang] = None\n",
    "\n",
    "# Clean up FP32 model\n",
    "del model_fp32\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nLoading INT8 quantized model...\")\n",
    "try:\n",
    "    model_int8 = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_8bit=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model_int8.eval()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"G2: QUANTIZATION EFFECTS - INT8\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    int8_results = {}\n",
    "    for lang, text in TEST_SENTENCES.items():\n",
    "        try:\n",
    "            ppl = compute_perplexity(model_int8, tokenizer, text)\n",
    "            int8_results[lang] = ppl\n",
    "            degradation = (ppl - fp32_results[lang]) / fp32_results[lang] * 100 if fp32_results[lang] else None\n",
    "            print(f\"{lang}: PPL = {ppl:.2f}, degradation = {degradation:+.1f}%\" if degradation else f\"{lang}: PPL = {ppl:.2f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{lang}: ERROR - {e}\")\n",
    "            int8_results[lang] = None\n",
    "    \n",
    "    del model_int8\n",
    "    torch.cuda.empty_cache()\n",
    "except Exception as e:\n",
    "    print(f\"INT8 loading failed: {e}\")\n",
    "    int8_results = {}\n",
    "\n",
    "# Compute degradation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"G2: DEGRADATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "degradation_results = {}\n",
    "for lang in TEST_SENTENCES.keys():\n",
    "    if fp32_results.get(lang) and int8_results.get(lang):\n",
    "        deg = (int8_results[lang] - fp32_results[lang]) / fp32_results[lang] * 100\n",
    "        degradation_results[lang] = {\n",
    "            'fp32_ppl': fp32_results[lang],\n",
    "            'int8_ppl': int8_results[lang],\n",
    "            'degradation_pct': deg,\n",
    "        }\n",
    "        print(f\"{lang}: {deg:+.1f}% degradation\")\n",
    "\n",
    "# Output in parseable format\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"G2_RESULTS_JSON_START\")\n",
    "print(json.dumps(degradation_results, indent=2))\n",
    "print(\"G2_RESULTS_JSON_END\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## G3: Layer Importance Probing\n",
    "\n",
    "**Question:** Are gateway layers (L0, Llast) more critical for low-resource languages?\n",
    "\n",
    "**Epistemic Value:** HIGH - tests architectural hypothesis\n",
    "\n",
    "**Method:**\n",
    "1. For each layer, apply noise/perturbation\n",
    "2. Measure perplexity degradation per language\n",
    "3. Compare layer importance profiles across languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === G3: LAYER IMPORTANCE PROBING ===\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "\n",
    "print(\"Loading model for layer probing...\")\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).cuda()\n",
    "model.eval()\n",
    "\n",
    "n_layers = model.config.n_layer\n",
    "print(f\"Model has {n_layers} layers\")\n",
    "\n",
    "# Test sentences\n",
    "TEST_SENTENCES = {\n",
    "    'en': \"The computer processes data quickly and efficiently.\",\n",
    "    'de': \"Der Computer verarbeitet Daten schnell und effizient.\",\n",
    "    'he': \"המחשב מעבד נתונים במהירות וביעילות.\",\n",
    "    'ar': \"يقوم الحاسوب بمعالجة البيانات بسرعة وكفاءة.\",\n",
    "}\n",
    "\n",
    "def compute_perplexity_with_noise(model, tokenizer, text, layer_idx, noise_scale=0.1):\n",
    "    \"\"\"Compute perplexity with noise added to specific layer.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    # Hook to add noise to layer output\n",
    "    noise_added = [False]\n",
    "    original_output = [None]\n",
    "    \n",
    "    def add_noise_hook(module, input, output):\n",
    "        if not noise_added[0]:\n",
    "            noise = torch.randn_like(output[0]) * noise_scale\n",
    "            noisy_output = output[0] + noise\n",
    "            noise_added[0] = True\n",
    "            return (noisy_output,) + output[1:]\n",
    "        return output\n",
    "    \n",
    "    # Register hook\n",
    "    layer = model.transformer.h[layer_idx]\n",
    "    handle = layer.register_forward_hook(add_noise_hook)\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "        ppl = torch.exp(loss).item()\n",
    "    finally:\n",
    "        handle.remove()\n",
    "    \n",
    "    return ppl\n",
    "\n",
    "def compute_baseline_ppl(model, tokenizer, text):\n",
    "    \"\"\"Compute baseline perplexity without noise.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "    \n",
    "    return torch.exp(loss).item()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"G3: LAYER IMPORTANCE PROBING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "layer_importance = {lang: {} for lang in TEST_SENTENCES}\n",
    "\n",
    "for lang, text in TEST_SENTENCES.items():\n",
    "    print(f\"\\nProcessing {lang}...\")\n",
    "    baseline_ppl = compute_baseline_ppl(model, tokenizer, text)\n",
    "    print(f\"  Baseline PPL: {baseline_ppl:.2f}\")\n",
    "    \n",
    "    for layer_idx in range(n_layers):\n",
    "        noisy_ppl = compute_perplexity_with_noise(model, tokenizer, text, layer_idx, noise_scale=0.1)\n",
    "        degradation = (noisy_ppl - baseline_ppl) / baseline_ppl * 100\n",
    "        layer_importance[lang][f\"L{layer_idx}\"] = {\n",
    "            'baseline_ppl': baseline_ppl,\n",
    "            'noisy_ppl': noisy_ppl,\n",
    "            'degradation_pct': degradation,\n",
    "        }\n",
    "    \n",
    "    # Print gateway vs middle comparison\n",
    "    gateway_layers = [0, n_layers-1]\n",
    "    middle_layers = list(range(2, n_layers-2))\n",
    "    \n",
    "    gateway_deg = np.mean([layer_importance[lang][f\"L{i}\"]['degradation_pct'] for i in gateway_layers])\n",
    "    middle_deg = np.mean([layer_importance[lang][f\"L{i}\"]['degradation_pct'] for i in middle_layers])\n",
    "    \n",
    "    print(f\"  Gateway layers (L0, L{n_layers-1}): {gateway_deg:.1f}% avg degradation\")\n",
    "    print(f\"  Middle layers: {middle_deg:.1f}% avg degradation\")\n",
    "    print(f\"  Gateway/Middle ratio: {gateway_deg/middle_deg:.2f}x\")\n",
    "\n",
    "# Output in parseable format\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"G3_RESULTS_JSON_START\")\n",
    "print(json.dumps(layer_importance, indent=2))\n",
    "print(\"G3_RESULTS_JSON_END\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Clean up\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Export\n",
    "\n",
    "Run this cell to get a complete summary of all results for copy-paste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FINAL SUMMARY ===\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GPU EXPERIMENT SUMMARY\")\n",
    "print(f\"Timestamp: {datetime.now().isoformat()}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None',\n",
    "    'experiments': ['G1_tokenization', 'G2_quantization', 'G3_layer_importance'],\n",
    "}\n",
    "\n",
    "# Add results if available\n",
    "try:\n",
    "    summary['G1_results'] = results  # From G1\n",
    "except:\n",
    "    summary['G1_results'] = 'Not run'\n",
    "\n",
    "try:\n",
    "    summary['G2_results'] = degradation_results  # From G2\n",
    "except:\n",
    "    summary['G2_results'] = 'Not run'\n",
    "\n",
    "try:\n",
    "    summary['G3_results'] = layer_importance  # From G3\n",
    "except:\n",
    "    summary['G3_results'] = 'Not run'\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPLETE_RESULTS_JSON_START\")\n",
    "print(json.dumps(summary, indent=2, default=str))\n",
    "print(\"COMPLETE_RESULTS_JSON_END\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nCopy everything between COMPLETE_RESULTS_JSON_START and COMPLETE_RESULTS_JSON_END\")\n",
    "print(\"and paste it back for local analysis.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
