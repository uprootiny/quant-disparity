{
  "experiment": "EXP-023",
  "configs": [
    {
      "model": "OPT-125M",
      "hidden_size": 768,
      "num_layers": 12,
      "num_heads": 12,
      "attention_softmax_in_fp32": "not_specified",
      "hidden_dropout_prob": 0.1,
      "attention_dropout": 0.0,
      "resid_dropout": "not_specified",
      "initializer_range": "not_specified",
      "scale_embedding": "not_specified",
      "tie_word_embeddings": "not_specified",
      "activation_function": "relu",
      "layer_norm_epsilon": "not_specified",
      "do_layer_norm_before": true,
      "word_embed_proj_dim": 768,
      "max_kurtosis": 562
    },
    {
      "model": "BLOOM-560M",
      "hidden_size": null,
      "num_layers": 24,
      "num_heads": 16,
      "attention_softmax_in_fp32": true,
      "hidden_dropout_prob": "not_specified",
      "attention_dropout": 0.0,
      "resid_dropout": "not_specified",
      "initializer_range": 0.02,
      "scale_embedding": "not_specified",
      "tie_word_embeddings": "not_specified",
      "activation_function": "not_specified",
      "layer_norm_epsilon": 1e-05,
      "do_layer_norm_before": "not_specified",
      "word_embed_proj_dim": "not_specified",
      "max_kurtosis": 504
    },
    {
      "model": "GPT-2-small",
      "hidden_size": 768,
      "num_layers": 12,
      "num_heads": 12,
      "attention_softmax_in_fp32": "not_specified",
      "hidden_dropout_prob": "not_specified",
      "attention_dropout": 0.1,
      "resid_dropout": 0.1,
      "initializer_range": 0.02,
      "scale_embedding": "not_specified",
      "tie_word_embeddings": "not_specified",
      "activation_function": "gelu_new",
      "layer_norm_epsilon": 1e-05,
      "do_layer_norm_before": "not_specified",
      "word_embed_proj_dim": "not_specified",
      "max_kurtosis": 201
    },
    {
      "model": "Pythia-410M",
      "hidden_size": 1024,
      "num_layers": 24,
      "num_heads": 16,
      "attention_softmax_in_fp32": "not_specified",
      "hidden_dropout_prob": "not_specified",
      "attention_dropout": "not_specified",
      "resid_dropout": "not_specified",
      "initializer_range": 0.02,
      "scale_embedding": "not_specified",
      "tie_word_embeddings": false,
      "activation_function": "gelu",
      "layer_norm_epsilon": 1e-05,
      "do_layer_norm_before": "not_specified",
      "word_embed_proj_dim": "not_specified",
      "max_kurtosis": 14
    },
    {
      "model": "XGLM-564M",
      "hidden_size": null,
      "num_layers": null,
      "num_heads": null,
      "attention_softmax_in_fp32": "not_specified",
      "hidden_dropout_prob": 0.1,
      "attention_dropout": 0.1,
      "resid_dropout": "not_specified",
      "initializer_range": "not_specified",
      "scale_embedding": true,
      "tie_word_embeddings": "not_specified",
      "activation_function": "gelu",
      "layer_norm_epsilon": "not_specified",
      "do_layer_norm_before": "not_specified",
      "word_embed_proj_dim": "not_specified",
      "max_kurtosis": 2
    }
  ],
  "verdict": "H2 SUPPORTED: dropout correlates with lower kurtosis"
}