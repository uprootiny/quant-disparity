{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP-003: Kurtosis-Degradation Analysis\n",
    "\n",
    "**Objective:** Validate that effective kurtosis (κ_eff) predicts quantization sensitivity.\n",
    "\n",
    "**Hypothesis H2:** r(κ_eff, D) is significant with |r| > 0.7\n",
    "\n",
    "**Theory:** Per Banner et al. (2019), optimal clipping threshold α* depends on weight distribution kurtosis:\n",
    "$$\\alpha^* = \\sigma \\cdot (2.5 + 0.3 \\cdot \\ln(1 + \\max(0, \\kappa)))$$\n",
    "\n",
    "**LA-ACIQ Extension:** Different languages activate different layers → different effective κ → different sensitivity.\n",
    "\n",
    "**References:**\n",
    "- Banner, R., et al. (2019). \"Post-Training 4-bit Quantization.\" NeurIPS.\n",
    "- Frühwirth-Schnatter, S. (2006). \"Finite Mixture Models.\" Springer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Setup\n",
    "!pip install -q transformers accelerate bitsandbytes scipy pandas matplotlib seaborn\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Configuration\n",
    "\n",
    "MODEL_NAME = \"bigscience/bloom-560m\"\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "# Languages with expected resource levels\n",
    "LANGUAGES = {\n",
    "    \"en\": {\"name\": \"English\", \"resource\": \"high\"},\n",
    "    \"fr\": {\"name\": \"French\", \"resource\": \"high\"},\n",
    "    \"zh\": {\"name\": \"Chinese\", \"resource\": \"high\"},\n",
    "    \"ar\": {\"name\": \"Arabic\", \"resource\": \"medium\"},\n",
    "    \"he\": {\"name\": \"Hebrew\", \"resource\": \"low\"},\n",
    "    \"sw\": {\"name\": \"Swahili\", \"resource\": \"low\"},\n",
    "}\n",
    "\n",
    "# Sample texts\n",
    "TEXTS = {\n",
    "    \"en\": \"The study of mathematics reveals the underlying structure of the universe and provides tools for understanding complex phenomena.\",\n",
    "    \"fr\": \"L'étude des mathématiques révèle la structure sous-jacente de l'univers et fournit des outils pour comprendre les phénomènes complexes.\",\n",
    "    \"zh\": \"数学研究揭示了宇宙的基本结构，并为理解复杂现象提供了工具。\",\n",
    "    \"ar\": \"دراسة الرياضيات تكشف البنية الأساسية للكون وتوفر أدوات لفهم الظواهر المعقدة.\",\n",
    "    \"he\": \"לימוד המתמטיקה חושף את המבנה הבסיסי של היקום ומספק כלים להבנת תופעות מורכבות.\",\n",
    "    \"sw\": \"Utafiti wa hisabati unaonyesha muundo wa msingi wa ulimwengu na kutoa zana za kuelewa matukio changamano.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model and Extract Layer Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load Model (FP16 for weight analysis)\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded. Layers: {model.config.num_hidden_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Extract Layer-wise Kurtosis\n",
    "\n",
    "def extract_layer_kurtosis(model) -> Dict[int, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Extract kurtosis statistics for each transformer layer.\n",
    "    \n",
    "    Returns dict mapping layer_idx -> {component: kurtosis}\n",
    "    \"\"\"\n",
    "    layer_stats = {}\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if \"transformer.h.\" not in name:\n",
    "            continue\n",
    "            \n",
    "        # Parse layer index\n",
    "        parts = name.split(\".\")\n",
    "        try:\n",
    "            layer_idx = int(parts[2])\n",
    "        except (IndexError, ValueError):\n",
    "            continue\n",
    "        \n",
    "        if layer_idx not in layer_stats:\n",
    "            layer_stats[layer_idx] = {\"weights\": [], \"kurtosis\": []}\n",
    "        \n",
    "        # Compute kurtosis for this parameter\n",
    "        weights = param.detach().cpu().float().numpy().flatten()\n",
    "        k = stats.kurtosis(weights, fisher=True)  # Excess kurtosis\n",
    "        layer_stats[layer_idx][\"kurtosis\"].append(k)\n",
    "        layer_stats[layer_idx][\"weights\"].append(len(weights))\n",
    "    \n",
    "    # Compute weighted average kurtosis per layer\n",
    "    result = {}\n",
    "    for layer_idx, data in layer_stats.items():\n",
    "        total_weights = sum(data[\"weights\"])\n",
    "        weighted_kurtosis = sum(\n",
    "            k * w / total_weights \n",
    "            for k, w in zip(data[\"kurtosis\"], data[\"weights\"])\n",
    "        )\n",
    "        result[layer_idx] = {\n",
    "            \"kurtosis\": weighted_kurtosis,\n",
    "            \"n_params\": total_weights,\n",
    "        }\n",
    "    \n",
    "    return dict(sorted(result.items()))\n",
    "\n",
    "layer_kurtosis = extract_layer_kurtosis(model)\n",
    "\n",
    "print(\"\\nLayer Kurtosis (excess):\")\n",
    "for layer, data in layer_kurtosis.items():\n",
    "    print(f\"  Layer {layer:2d}: κ = {data['kurtosis']:7.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Identify Outlier Layers\n",
    "\n",
    "kurtosis_values = [d[\"kurtosis\"] for d in layer_kurtosis.values()]\n",
    "mean_k = np.mean(kurtosis_values)\n",
    "std_k = np.std(kurtosis_values)\n",
    "\n",
    "# Outlier threshold: κ > mean + 2*std\n",
    "outlier_threshold = mean_k + 2 * std_k\n",
    "\n",
    "outlier_layers = [\n",
    "    layer for layer, data in layer_kurtosis.items() \n",
    "    if data[\"kurtosis\"] > outlier_threshold\n",
    "]\n",
    "\n",
    "print(f\"\\nKurtosis statistics:\")\n",
    "print(f\"  Mean: {mean_k:.2f}\")\n",
    "print(f\"  Std: {std_k:.2f}\")\n",
    "print(f\"  Max: {max(kurtosis_values):.2f}\")\n",
    "print(f\"  Outlier threshold: {outlier_threshold:.2f}\")\n",
    "print(f\"  Outlier layers: {outlier_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Measure Language-Specific Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Capture Layer Activations\n",
    "\n",
    "def get_layer_activations(model, tokenizer, text: str) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Capture activation magnitudes per layer for a given text.\n",
    "    \n",
    "    Returns: dict mapping layer_idx -> mean activation magnitude\n",
    "    \"\"\"\n",
    "    activations = {}\n",
    "    hooks = []\n",
    "    \n",
    "    def make_hook(layer_idx):\n",
    "        def hook(module, input, output):\n",
    "            # output is (hidden_states, ...)\n",
    "            hidden = output[0] if isinstance(output, tuple) else output\n",
    "            activations[layer_idx] = hidden.abs().mean().item()\n",
    "        return hook\n",
    "    \n",
    "    # Register hooks\n",
    "    for i, layer in enumerate(model.transformer.h):\n",
    "        hooks.append(layer.register_forward_hook(make_hook(i)))\n",
    "    \n",
    "    # Forward pass\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model(**inputs)\n",
    "    \n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    return activations\n",
    "\n",
    "# Collect activations for all languages\n",
    "language_activations = {}\n",
    "for lang, text in TEXTS.items():\n",
    "    activations = get_layer_activations(model, tokenizer, text)\n",
    "    language_activations[lang] = activations\n",
    "    print(f\"{lang}: collected {len(activations)} layer activations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Compute Effective Kurtosis per Language\n",
    "\n",
    "def compute_effective_kurtosis(\n",
    "    layer_kurtosis: Dict[int, Dict], \n",
    "    activations: Dict[int, float]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute effective kurtosis as activation-weighted average.\n",
    "    \n",
    "    κ_eff(λ) = Σ_l ā_l(λ) · κ_l\n",
    "    \n",
    "    where ā_l is the normalized activation for layer l.\n",
    "    \"\"\"\n",
    "    total_activation = sum(activations.values())\n",
    "    if total_activation == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    weighted_sum = sum(\n",
    "        (activations[l] / total_activation) * layer_kurtosis[l][\"kurtosis\"]\n",
    "        for l in activations.keys()\n",
    "        if l in layer_kurtosis\n",
    "    )\n",
    "    \n",
    "    return weighted_sum\n",
    "\n",
    "# Compute κ_eff for each language\n",
    "effective_kurtosis = {}\n",
    "for lang, activations in language_activations.items():\n",
    "    k_eff = compute_effective_kurtosis(layer_kurtosis, activations)\n",
    "    effective_kurtosis[lang] = k_eff\n",
    "    print(f\"{lang}: κ_eff = {k_eff:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Measure Quantization Degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load INT4 Model and Measure Degradation\n",
    "\n",
    "# Clear cache\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load FP16 for baseline\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model_fp16.eval()\n",
    "\n",
    "# Load INT4\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "model_int4 = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model_int4.eval()\n",
    "\n",
    "print(\"Both models loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Compute Perplexity and Degradation\n",
    "\n",
    "def compute_perplexity(model, tokenizer, text: str) -> float:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        return torch.exp(outputs.loss).item()\n",
    "\n",
    "# Measure for all languages\n",
    "results = []\n",
    "for lang, text in TEXTS.items():\n",
    "    ppl_fp16 = compute_perplexity(model_fp16, tokenizer, text)\n",
    "    ppl_int4 = compute_perplexity(model_int4, tokenizer, text)\n",
    "    degradation = (ppl_int4 - ppl_fp16) / ppl_fp16\n",
    "    \n",
    "    results.append({\n",
    "        \"lang\": lang,\n",
    "        \"name\": LANGUAGES[lang][\"name\"],\n",
    "        \"resource\": LANGUAGES[lang][\"resource\"],\n",
    "        \"k_eff\": effective_kurtosis[lang],\n",
    "        \"ppl_fp16\": ppl_fp16,\n",
    "        \"ppl_int4\": ppl_int4,\n",
    "        \"degradation\": degradation,\n",
    "    })\n",
    "    \n",
    "    print(f\"{lang}: κ_eff={effective_kurtosis[lang]:.3f}, D={degradation:.4f}\")\n",
    "\n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Test H2: Kurtosis-Degradation Correlation\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"HYPOTHESIS H2: κ_eff correlates with degradation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Pearson correlation\n",
    "r_pearson, p_pearson = stats.pearsonr(df[\"k_eff\"], df[\"degradation\"])\n",
    "\n",
    "# Spearman correlation (robust to non-normality)\n",
    "r_spearman, p_spearman = stats.spearmanr(df[\"k_eff\"], df[\"degradation\"])\n",
    "\n",
    "print(f\"\\nPearson correlation:\")\n",
    "print(f\"  r = {r_pearson:.4f}\")\n",
    "print(f\"  p = {p_pearson:.4f}\")\n",
    "\n",
    "print(f\"\\nSpearman correlation:\")\n",
    "print(f\"  ρ = {r_spearman:.4f}\")\n",
    "print(f\"  p = {p_spearman:.4f}\")\n",
    "\n",
    "# Hypothesis evaluation\n",
    "h2_supported = abs(r_pearson) > 0.7 and p_pearson < 0.05\n",
    "print(f\"\\nH2 Result: {'SUPPORTED' if h2_supported else 'NOT SUPPORTED'}\")\n",
    "print(f\"  Criterion: |r| > 0.7 and p < 0.05\")\n",
    "print(f\"  Observed: |r| = {abs(r_pearson):.4f}, p = {p_pearson:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Visualization\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Layer kurtosis profile\n",
    "ax1 = axes[0]\n",
    "layers = list(layer_kurtosis.keys())\n",
    "kurtosis = [layer_kurtosis[l][\"kurtosis\"] for l in layers]\n",
    "colors = ['red' if l in outlier_layers else 'steelblue' for l in layers]\n",
    "ax1.bar(layers, kurtosis, color=colors)\n",
    "ax1.axhline(outlier_threshold, color='red', linestyle='--', label=f'Outlier threshold: {outlier_threshold:.1f}')\n",
    "ax1.set_xlabel(\"Layer\")\n",
    "ax1.set_ylabel(\"Excess Kurtosis\")\n",
    "ax1.set_title(\"Layer Kurtosis Profile\")\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: κ_eff vs Degradation\n",
    "ax2 = axes[1]\n",
    "colors_resource = {\"high\": \"#2ecc71\", \"medium\": \"#f39c12\", \"low\": \"#e74c3c\"}\n",
    "for _, row in df.iterrows():\n",
    "    ax2.scatter(row[\"k_eff\"], row[\"degradation\"], \n",
    "                c=colors_resource[row[\"resource\"]], s=100, label=row[\"lang\"])\n",
    "    ax2.annotate(row[\"lang\"], (row[\"k_eff\"], row[\"degradation\"]), \n",
    "                 xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "# Regression line\n",
    "z = np.polyfit(df[\"k_eff\"], df[\"degradation\"], 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(df[\"k_eff\"].min(), df[\"k_eff\"].max(), 100)\n",
    "ax2.plot(x_line, p(x_line), 'k--', alpha=0.5)\n",
    "\n",
    "ax2.set_xlabel(\"Effective Kurtosis (κ_eff)\")\n",
    "ax2.set_ylabel(\"Degradation\")\n",
    "ax2.set_title(f\"H2: κ_eff vs Degradation (r={r_pearson:.3f})\")\n",
    "\n",
    "# Plot 3: Results table\n",
    "ax3 = axes[2]\n",
    "ax3.axis('off')\n",
    "table_data = df[[\"lang\", \"k_eff\", \"degradation\"]].round(4).values\n",
    "table = ax3.table(\n",
    "    cellText=table_data,\n",
    "    colLabels=[\"Lang\", \"κ_eff\", \"Degradation\"],\n",
    "    loc='center',\n",
    "    cellLoc='center'\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.5)\n",
    "ax3.set_title(\"Results Summary\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"exp003_results.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Save Results\n",
    "\n",
    "results_summary = {\n",
    "    \"experiment\": \"EXP-003: Kurtosis-Degradation Analysis\",\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"hypothesis\": {\n",
    "        \"H2\": {\n",
    "            \"statement\": \"r(κ_eff, D) significant with |r| > 0.7\",\n",
    "            \"r_pearson\": round(r_pearson, 4),\n",
    "            \"p_pearson\": round(p_pearson, 4),\n",
    "            \"r_spearman\": round(r_spearman, 4),\n",
    "            \"p_spearman\": round(p_spearman, 4),\n",
    "            \"supported\": h2_supported,\n",
    "        }\n",
    "    },\n",
    "    \"layer_statistics\": {\n",
    "        \"mean_kurtosis\": round(mean_k, 4),\n",
    "        \"std_kurtosis\": round(std_k, 4),\n",
    "        \"max_kurtosis\": round(max(kurtosis_values), 4),\n",
    "        \"outlier_layers\": outlier_layers,\n",
    "    },\n",
    "    \"per_language\": df.to_dict(orient=\"records\"),\n",
    "}\n",
    "\n",
    "with open(\"exp003_results.json\", \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nH2: {'SUPPORTED' if h2_supported else 'NOT SUPPORTED'}\")\n",
    "print(f\"Correlation: r = {r_pearson:.4f} (p = {p_pearson:.4f})\")\n",
    "print(f\"\\nResults saved to exp003_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interpretation\n",
    "\n",
    "### Theory Connection\n",
    "\n",
    "The effective kurtosis κ_eff captures how heavy-tailed the weight distribution \"appears\" to a given language based on its activation pattern.\n",
    "\n",
    "**If correlation is NEGATIVE (r < 0):**\n",
    "- Higher κ_eff → LESS degradation\n",
    "- This makes sense: Banner's formula gives larger α* for higher κ\n",
    "- Global α* is tuned for average κ\n",
    "- Languages with above-average κ_eff benefit (less clipping)\n",
    "- Languages with below-average κ_eff suffer (too much clipping)\n",
    "\n",
    "**If correlation is POSITIVE (r > 0):**\n",
    "- Higher κ_eff → MORE degradation\n",
    "- This would contradict the simple theory\n",
    "- May indicate other factors dominate\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Small sample (6 languages)\n",
    "- Single text per language\n",
    "- κ_eff is approximation (activation-weighted, not true mixture)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
