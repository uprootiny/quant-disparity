{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03: Real GPTQ Quantization Test\n",
        "\n",
        "**Goal:** Test if our findings apply to actual GPTQ (not simulated INT4)\n",
        "\n",
        "**Time:** ~45 minutes on T4\n",
        "\n",
        "**Key question:** Does GPTQ's calibration-based optimization already handle disparity?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate auto-gptq optimum\n",
        "\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "from src.disparity import measure_disparity, perplexity, DEFAULT_TEXTS\n",
        "from src.utils import print_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pre-quantized GPTQ model\n",
        "GPTQ_MODEL = \"TheBloke/Llama-2-7B-GPTQ\"\n",
        "\n",
        "print(f\"Loading {GPTQ_MODEL}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(GPTQ_MODEL)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoGPTQForCausalLM.from_quantized(\n",
        "    GPTQ_MODEL,\n",
        "    device=\"cuda:0\",\n",
        "    use_triton=False,\n",
        ")\n",
        "print(\"GPTQ model loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Also load FP16 baseline for comparison\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "FP16_MODEL = \"NousResearch/Llama-2-7b-hf\"\n",
        "print(f\"Loading {FP16_MODEL} for baseline...\")\n",
        "\n",
        "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
        "    FP16_MODEL,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model_fp16.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEXTS = {k: v for k, v in DEFAULT_TEXTS.items() if k in ['en', 'he', 'ar', 'zh', 'de', 'fr']}\n",
        "\n",
        "# Baseline (FP16)\n",
        "print(\"Computing FP16 baseline...\")\n",
        "baseline = {lang: perplexity(model_fp16, tokenizer, text) for lang, text in TEXTS.items()}\n",
        "print(f\"Baseline: en={baseline['en']:.1f}\")\n",
        "\n",
        "# Free FP16 model\n",
        "del model_fp16\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Measure GPTQ disparity\n",
        "print(\"\\nMeasuring GPTQ disparity...\")\n",
        "gptq_results = measure_disparity(model, tokenizer, TEXTS, baseline)\n",
        "\n",
        "print_results(gptq_results, \"GPTQ Quantization Results\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interpretation\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"INTERPRETATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "avg_disp = gptq_results['avg_disparity']\n",
        "\n",
        "if avg_disp < 2.0:\n",
        "    print(f\"\"\"\n",
        "✓ GPTQ shows low disparity ({avg_disp:.2f}x)\n",
        "\n",
        "This suggests GPTQ's calibration-based optimization may partially\n",
        "address multilingual disparity. However:\n",
        "\n",
        "1. Calibration data matters - was it English-only or multilingual?\n",
        "2. Our layer protection insight still valuable for understanding WHY\n",
        "3. Further experiment: Compare English-only vs multilingual calibration\n",
        "\"\"\")\n",
        "elif avg_disp < 10.0:\n",
        "    print(f\"\"\"\n",
        "~ GPTQ shows moderate disparity ({avg_disp:.2f}x)\n",
        "\n",
        "GPTQ reduces but doesn't eliminate disparity. Our findings suggest:\n",
        "\n",
        "1. Layer protection could further improve GPTQ\n",
        "2. Multilingual calibration might help\n",
        "3. Hybrid approach: GPTQ + layer protection\n",
        "\"\"\")\n",
        "else:\n",
        "    print(f\"\"\"\n",
        "✗ GPTQ shows high disparity ({avg_disp:.2f}x)\n",
        "\n",
        "GPTQ doesn't solve the multilingual disparity problem. Our findings\n",
        "are directly applicable:\n",
        "\n",
        "1. Layer protection strategy adds value\n",
        "2. Consider layer-aware GPTQ modification\n",
        "3. Practical contribution for multilingual deployment\n",
        "\"\"\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
