{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01: Quick Validation - Does L0+L31 Work on Llama?\n",
        "\n",
        "**Goal:** Test if GPT-2's gateway layer pattern transfers to Llama-2-7B\n",
        "\n",
        "**Time:** ~30 minutes on T4\n",
        "\n",
        "**Key question:** Does protecting input (L0) + output (L31) layers reduce multilingual disparity?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "!pip install -q transformers accelerate\n",
        "\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from src.disparity import measure_disparity, DEFAULT_TEXTS\n",
        "from src.quantize import simulate_int4, save_state, restore_model, get_num_layers\n",
        "from src.utils import print_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model (use NousResearch version - no approval needed)\n",
        "MODEL_ID = \"NousResearch/Llama-2-7b-hf\"\n",
        "\n",
        "print(f\"Loading {MODEL_ID}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "num_layers = get_num_layers(model)\n",
        "print(f\"Model loaded: {num_layers} layers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test texts (subset for speed)\n",
        "TEXTS = {\n",
        "    'en': DEFAULT_TEXTS['en'],\n",
        "    'he': DEFAULT_TEXTS['he'],\n",
        "    'ar': DEFAULT_TEXTS['ar'],\n",
        "    'zh': DEFAULT_TEXTS['zh'],\n",
        "}\n",
        "\n",
        "# Save state for restoration\n",
        "state = save_state(model)\n",
        "\n",
        "# Baseline perplexity\n",
        "from src.disparity import perplexity\n",
        "baseline = {lang: perplexity(model, tokenizer, text) for lang, text in TEXTS.items()}\n",
        "print(\"Baseline PPL (FP16):\")\n",
        "for lang, ppl in sorted(baseline.items(), key=lambda x: x[1]):\n",
        "    print(f\"  {lang}: {ppl:.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test configurations\n",
        "# GPT-2: L0+L11 (12 layers) → Llama: L0+L31 (32 layers)\n",
        "# GPT-2: L0+L9+L11 → Llama: L0+L24+L31 (75% = 24)\n",
        "\n",
        "CONFIGS = {\n",
        "    \"no_protection\": [],\n",
        "    \"L0+L31 (GPT-2 equivalent)\": [0, 31],\n",
        "    \"L0+L24+L31 (with 75% layer)\": [0, 24, 31],\n",
        "    \"L0+L16+L31 (with 50% layer)\": [0, 16, 31],\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for config_name, protect in CONFIGS.items():\n",
        "    print(f\"\\nTesting: {config_name}\")\n",
        "    restore_model(model, state)\n",
        "    simulate_int4(model, exclude=set(protect))\n",
        "    \n",
        "    metrics = measure_disparity(model, tokenizer, TEXTS, baseline)\n",
        "    results[config_name] = metrics\n",
        "    \n",
        "    print(f\"  Avg disparity: {metrics['avg_disparity']:.2f}x\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\n{'Config':<35} {'Avg Disparity':>15}\")\n",
        "print(\"-\" * 52)\n",
        "for config, metrics in results.items():\n",
        "    print(f\"{config:<35} {metrics['avg_disparity']:>14.2f}x\")\n",
        "\n",
        "# Interpretation\n",
        "best_config = min(results.items(), key=lambda x: x[1]['avg_disparity'])\n",
        "print(f\"\\nBest config: {best_config[0]}\")\n",
        "print(f\"Disparity: {best_config[1]['avg_disparity']:.2f}x\")\n",
        "\n",
        "if best_config[1]['avg_disparity'] < 2.0:\n",
        "    print(\"\\n✓ PATTERN TRANSFERS: Gateway layer protection works on Llama!\")\n",
        "else:\n",
        "    print(\"\\n✗ PATTERN DIFFERS: Need full layer sweep to find Llama's critical layers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-language breakdown for best config\n",
        "print_results(best_config[1], f\"Best Config: {best_config[0]}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
